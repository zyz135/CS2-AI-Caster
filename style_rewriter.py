import pandas as pd
import os
import json
import glob
import concurrent.futures
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================
MY_API_KEY = "sk-c2435a4ac2574b4e8ef61ef0c3da7ed4"  # ä½ çš„ Key
MODEL_NAME = "qwen3-max" 
BLACKLIST_KEYWORDS = [
    "æ‘”æ­»", "è‡ªæ€", "æœªçŸ¥", "world", "World", "Trigger", "entity", "Bot", "BOT"
]
# ===========================================

def get_machine_style_prompt_with_timing(events_batch):
    """
    æ„å»ºå¸¦æœ‰æ—¶é•¿çº¦æŸçš„ Prompt
    events_batch ç»“æ„: [{"id": 0, "text": "åŸæ–‡...", "duration": 4.0}, ...]
    """
    events_str = json.dumps(events_batch, ensure_ascii=False)
    
    return f"""
ä½ ç°åœ¨æ˜¯CS2çŸ¥åä¸»æ’­â€œç©æœºå™¨â€ï¼ˆMachineï¼‰ã€‚
è¯·å°†ä»¥ä¸‹ã€è§£è¯´åŸæ–‡ã€‘é‡å†™ä¸ºã€ç©æœºå™¨ç›´æ’­é£æ ¼ã€‘ï¼Œå¹¶**ä¸¥æ ¼éµå®ˆæ—¶é•¿é™åˆ¶**ã€‚

ã€äººè®¾è¦æ±‚ã€‘
1. **é£æ ¼**ï¼šé˜´é˜³æ€ªæ°”ã€ç©æ¢—ï¼ˆFlameZ=ç«ä»”, ZywOo=è½½ç‰©/è–¯è–¯ï¼Œzweih=å“²ä¼Ÿï¼Œæªæ¢°ç”¨ç®€ç§°ï¼Œæ‹’ç»æœºæ¢°æ„Ÿã€‚
2. **æ ¸å¿ƒçº¦æŸ**ï¼šæ¯æ¡æ•°æ®éƒ½æœ‰ "duration" (ç§’)ã€‚**ä¸»æ’­è¯­é€Ÿçº¦ä¸º 5å­—/ç§’**ã€‚
   - å¦‚æœ duration=3.0sï¼Œå­—æ•°ä¸èƒ½è¶…è¿‡ 15 å­—ã€‚
   - å¦‚æœ duration=8.0sï¼Œå¯ä»¥å±•å¼€è®²ï¼Œä½†ä¸è¦åºŸè¯ã€‚
   - **è¶…æ—¶æ˜¯è§£è¯´çš„å¤§å¿Œï¼å¿…é¡»ç²¾ç®€ï¼**
3. **è¿‡æ»¤**ï¼šé‡åˆ°â€œæ‘”æ­»/è‡ªæ€â€ç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸² ""ã€‚
4. é—ªå…‰å¼¹å°±å«é—ªæˆ–é—ªå…‰ï¼ˆä¸è¦å«é—ªbangï¼‰ï¼ŒçƒŸé›¾å¼¹å«çƒŸï¼Œç‡ƒçƒ§å¼¹å’Œç‡ƒçƒ§ç“¶å«ç«ï¼ŒAK-47å«AKï¼ŒM4A4å«A4ï¼ŒM4A1-Så«A1ï¼Œ
AWPå«å¤§ç‹™ï¼Œæ²™æ¼ ä¹‹é¹°å«æ²™é¹°ï¼Œssg-08å«é¸Ÿç‹™ï¼Œä»¥åŠä¸€äº›å¸¸ç”¨çš„è¯´æ³•å’Œæ˜µç§°ä¸Šç½‘æœå¯»
5. è¯´æ˜é€‰æ‰‹æ‰€åœ¨ç‚¹ä½ç”¨ç‚¹ä½åå°±è¡Œäº†ï¼Œä¸è¦å‡ºç°â€œç¼“å†²åŒºâ€ç­‰æœºæ¢°æ„Ÿå­—çœ¼
ã€è¾“å…¥æ•°æ®ã€‘(JSON):
{events_str}

ã€è¾“å‡ºæ ¼å¼ã€‘(çº¯JSONåˆ—è¡¨ï¼ŒåªåŒ…å«é‡å†™åçš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œé¡ºåºå¯¹åº”):
["é‡å†™åçš„çŸ­å¥(3s)", "é‡å†™åçš„é•¿å¥(8s)", ""]
"""

def calculate_duration(time_range_str):
    """ä» '15.0-19.0s' ä¸­è®¡ç®—æ—¶é•¿"""
    try:
        t_str = str(time_range_str).replace("s", "").strip()
        start, end = t_str.split('-')
        return float(end) - float(start)
    except:
        return 4.0 # é»˜è®¤å…œåº•

def process_batch(client, batch_df):
    """å¤„ç†æ‰¹æ¬¡"""
    # 1. æ„å»ºåŒ…å«æ—¶é•¿çš„è¾“å…¥ç»“æ„
    input_events = []
    original_texts = batch_df['è§£è¯´æ–‡æœ¬'].tolist()
    
    for idx, row in batch_df.iterrows():
        text = str(row['è§£è¯´æ–‡æœ¬'])
        duration = calculate_duration(row['æ—¶é—´èŒƒå›´'])
        
        # é¢„è¿‡æ»¤
        if any(bad in text for bad in BLACKLIST_KEYWORDS):
            text = "SKIP_THIS_EVENT"
            
        input_events.append({
            "text": text,
            "duration": round(duration, 1) # ä¿ç•™1ä½å°æ•°
        })

    # å¦‚æœå…¨æ˜¯è¢«è¿‡æ»¤çš„
    if all(e["text"] == "SKIP_THIS_EVENT" for e in input_events):
        return [""] * len(input_events)

    # 2. è°ƒç”¨ LLM
    try:
        prompt = get_machine_style_prompt_with_timing(input_events)
        
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8,
            response_format={"type": "json_object"} if "qwen" not in MODEL_NAME else None
        )
        
        content = response.choices[0].message.content
        content = content.replace("```json", "").replace("```", "").strip()
        
        try:
            new_texts = json.loads(content)
        except:
            # ç®€å•çš„åˆ—è¡¨è¡¥æ•‘
            import ast
            if "[" in content:
                start = content.find("[")
                end = content.rfind("]") + 1
                new_texts = ast.literal_eval(content[start:end])
            else:
                return original_texts

        # é•¿åº¦å¯¹é½
        if len(new_texts) != len(original_texts):
            # ç®€å•çš„æˆªæ–­æˆ–è¡¥é½
            if len(new_texts) > len(original_texts):
                new_texts = new_texts[:len(original_texts)]
            else:
                new_texts.extend([""] * (len(original_texts) - len(new_texts)))
                
        return new_texts

    except Exception as e:
        print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
        return original_texts

def process_file(filepath):
    print(f"\nâ±ï¸  æ­£åœ¨è¿›è¡Œ[é™æ—¶]é£æ ¼åŒ–é‡å†™: {os.path.basename(filepath)} ...")
    try:
        df = pd.read_csv(filepath, encoding='utf-8-sig')
        if df.empty: return

        # åŒæ ·æ’é™¤å·²å¤„ç†æ–‡ä»¶
        if "_machine_style" in filepath: return

        client = OpenAI(api_key=MY_API_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        
        batch_size = 8 # æ‰¹æ¬¡ç¨å¾®å°ä¸€ç‚¹ï¼Œè®©æ¨¡å‹æ›´ä¸“æ³¨
        batches = [df[i:i + batch_size] for i in range(0, df.shape[0], batch_size)]
        
        styled_texts = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_batch = {executor.submit(process_batch, client, batch): i for i, batch in enumerate(batches)}
            
            results_map = {}
            for future in concurrent.futures.as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    res = future.result()
                    results_map[batch_idx] = res
                    print(f"   âœ… æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} å®Œæˆ")
                except Exception as e:
                    print(f"   âŒ æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    results_map[batch_idx] = batches[batch_idx]['è§£è¯´æ–‡æœ¬'].tolist()

        for i in range(len(batches)):
            styled_texts.extend(results_map.get(i, []))
            
        df['åŸè§£è¯´'] = df['è§£è¯´æ–‡æœ¬']
        df['è§£è¯´æ–‡æœ¬'] = styled_texts
        
        # è¿‡æ»¤ç©ºè¡Œ
        df_final = df[df['è§£è¯´æ–‡æœ¬'].astype(str).str.strip() != ""]
        
        new_path = filepath.replace(".csv", "_machine_style.csv")
        df_final.to_csv(new_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ‰ å®Œç¾ï¼è¾“å‡º: {new_path}")
        
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")

def main():
    print("ğŸ” æ‰«æ data ç›®å½•...")
    search_pattern = os.path.join("data", "**", "final_*_half.csv")
    files = glob.glob(search_pattern, recursive=True)
    target_files = [f for f in files if "_machine_style" not in f]
    
    for f in target_files:
        process_file(f)

if __name__ == "__main__":
    main()