import pandas as pd
import os
import json
import re
import concurrent.futures
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
MY_API_KEY = os.getenv("DASHSCOPE_API_KEY") 
# å»ºè®®ä½¿ç”¨ qwen-max ä»¥è·å¾—æ›´å¥½çš„é£æ ¼éµå¾ªèƒ½åŠ›ï¼Œå¦‚æœæƒ³çœé’±å¯ä»¥ç”¨ qwen-plus
MODEL_NAME = "qwen3-max" 
BATCH_SIZE = 10  # æ‰¹å¤„ç†å¤§å°
TARGET_SPEED = 3.5 # ç›®æ ‡è¯­é€Ÿï¼šæ¯ç§’ X ä¸ªå­— (ç©æœºå™¨è¯­é€Ÿç¨å¿«ï¼Œè®¾ä¸º3.5æ¯”è¾ƒè‡ªç„¶)

# é»‘åå•å…³é”®è¯ï¼ˆå¦‚æœåŸæ–‡åŒ…å«è¿™äº›ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†æˆ–è¿‡æ»¤ï¼‰
BLACKLIST_KEYWORDS = [
    "æ‘”æ­»", "è‡ªæ€", "æœªçŸ¥", "world", "World", "Trigger", "entity", "Bot", "BOT"
]
# ===========================================

def parse_duration(time_range_str):
    """ä» '10.5-15.2s' æ ¼å¼ä¸­è§£ææŒç»­æ—¶é—´"""
    try:
        clean = str(time_range_str).lower().replace("s", "").strip()
        start, end = clean.split("-")
        duration = float(end) - float(start)
        return max(1.5, duration) # æœ€å°‘ç»™1.5ç§’ï¼Œé˜²æ­¢é™¤ä»¥0æˆ–è¿‡çŸ­
    except:
        return 4.0 # é»˜è®¤å…œåº•æ—¶é•¿

def get_machine_style_prompt(events_batch):
    """
    æ„å»ºåŒ…å«ã€è¯­é€Ÿé™åˆ¶ã€‘çš„ç©æœºå™¨é£æ ¼ Prompt
    """
    # å°† DataFrame è¡Œè½¬ä¸ºå­—å…¸åˆ—è¡¨ï¼Œå¹¶æ³¨å…¥å­—æ•°é™åˆ¶
    context_data = []
    for item in events_batch:
        duration = item['duration']
        # è®¡ç®—ç›®æ ‡å­—æ•°ï¼šæ—¶é•¿ * è¯­é€Ÿï¼Œä¸Šä¸‹æµ®åŠ¨ä¸€ç‚¹
        target_len = int(duration * TARGET_SPEED)
        
        context_data.append({
            "id": item['idx'],
            "åŸæ–‡": item['text'],
            "æ—¶é•¿": f"{duration:.1f}ç§’",
            "é™åˆ¶": f"{target_len}å­—å·¦å³"  # æ˜¾å¼å‘Šè¯‰LLMå­—æ•°é™åˆ¶
        })

    events_str = json.dumps(context_data, ensure_ascii=False, indent=2)
    
    return f"""
ä½ ç°åœ¨æ˜¯CS2çŸ¥åä¸»æ’­â€œç©æœºå™¨â€ï¼ˆMachineï¼‰ã€‚
è¯·å°†ä»¥ä¸‹ã€è§£è¯´åŸæ–‡ã€‘é‡å†™ä¸ºã€ç©æœºå™¨ç›´æ’­é£æ ¼ã€‘ã€‚

ã€âš ï¸âš ï¸ æ ¸å¿ƒè¦æ±‚ï¼šè¯­é€Ÿæ§åˆ¶ âš ï¸âš ï¸ã€‘
1. **ä¸¥æ ¼éµå®ˆå­—æ•°é™åˆ¶**ï¼šæ¯æ¡æ•°æ®éƒ½æ ‡æ³¨äº†`é™åˆ¶`ï¼ˆåŸºäº3.5å­—/ç§’è®¡ç®—ï¼‰ã€‚
   - å¦‚æœæ—¶é•¿åªæœ‰ 2ç§’ï¼Œä½ åªèƒ½è¯´ 6-7 ä¸ªå­—ï¼(ä¾‹å¦‚ï¼š"è¿™æ³¢donkç›´æ¥ç§’äº†ï¼")
   - ç»ä¸è¦å†™é•¿ï¼è§£è¯´å¿…é¡»è·Ÿå¾—ä¸Šç”»é¢ï¼
   - å¦‚æœåŸæ–‡å¾ˆé•¿ä½†æ—¶é—´å¾ˆçŸ­ï¼Œ**å¿…é¡»å¤§å¹…åˆ å‡**ï¼Œåªç•™æœ€æ ¸å¿ƒçš„å‡»æ€ä¿¡æ¯ã€‚

ã€äººè®¾é£æ ¼ã€‘
1. **å£è¯­åŒ–/é€ æ¢—**ï¼š
   - æ‹’ç»æœºæ¢°æ’­æŠ¥ï¼ä¸è¦è¯´"ç©å®¶Aå‡»æ€äº†ç©å®¶B"ï¼Œè¦è¯´"Aè¿™æ³¢å®šä½å¤ªå‡†äº†"ã€"Bç›´æ¥ç™½ç»™"ã€‚
   - å¸¸ç”¨è¯ï¼šå¹²æ‹‰ã€ç™½ç»™ã€è¿™æ³¢ã€æœ‰ç‚¹ä¸œè¥¿ã€ä¹Ÿæ˜¯æ²¡è°äº†ã€è¿™æŠŠæ²¡äº†ã€‚
   - ç§°å‘¼ï¼šZywOo=è½½ç‰©, s1mple=æ£®ç ´, NiKo=å°¼å…¬å­, donk=ä¸œé›ª, m0NESY=å°å­©ã€‚
2. **æƒ…ç»ª**ï¼š
   - çœ‹åˆ°è¿æ€è¦æ¿€åŠ¨ï¼šâ€œå§æ§½ï¼è¿™ä¹Ÿèƒ½æ€ï¼Ÿï¼â€
   - çœ‹åˆ°å¤±è¯¯è¦åæ§½ï¼šâ€œè¿™æ³¢ä»–åœ¨å¹²å˜›ï¼Ÿä»–åœ¨é©¬ä»€ä¹ˆï¼Ÿâ€

ã€è¾“å…¥æ•°æ®ã€‘
{events_str}

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒKeyæ˜¯idï¼ŒValueæ˜¯é‡å†™åçš„æ–‡æœ¬ã€‚
ä¾‹å¦‚ï¼š
{{
  "0": "è¿™æ³¢è½½ç‰©å¤§ç‹™æ¶å¾—å¤ªæ­»ï¼Œæ²¡äººèƒ½è¿‡ï¼",
  "1": "donkè¿™å°±ç›´æ¥å¹²æ‹‰äº†ï¼Ÿå¤ªè‡ªä¿¡äº†å§ï¼"
}}
"""

def process_batch(client, batch_df):
    """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
    # å‡†å¤‡æ•°æ®ï¼ŒåŒ…å«ç´¢å¼•ä»¥ä¾¿å¯¹åº”
    batch_input = []
    for idx, row in batch_df.iterrows():
        text = str(row.get('è§£è¯´æ–‡æœ¬', ''))
        if not text or text.lower() == 'nan': continue
        
        duration = parse_duration(row.get('æ—¶é—´èŒƒå›´', '0-0s'))
        
        batch_input.append({
            "idx": idx,
            "text": text,
            "duration": duration
        })
    
    if not batch_input: return {}

    prompt = get_machine_style_prompt(batch_input)
    
    try:
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼æ§åˆ¶è¯­é€Ÿçš„CS2è§£è¯´ã€‚è¯·ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦åŒ…å«markdownæ ‡è®°ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8, # ç¨å¾®é«˜ä¸€ç‚¹ï¼Œå¢åŠ é£æ ¼åŒ–
            response_format={"type": "json_object"}
        )
        content = resp.choices[0].message.content
        # æ¸…æ´—å¯èƒ½çš„ markdown
        if content.startswith("```json"): content = content[7:-3]
        
        return json.loads(content)
    except Exception as e:
        print(f"   âš ï¸ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
        # å¤±è´¥å…œåº•ï¼šè¿”å›åŸæ–‡
        return {item['idx']: item['text'] for item in batch_input}

def process_file(csv_path):
    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    print(f"âœ¨ [Style] æ­£åœ¨ç©æœºå™¨åŒ– (3.5å­—/s): {os.path.basename(csv_path)}")
    df = pd.read_csv(csv_path)
    
    if 'è§£è¯´æ–‡æœ¬' not in df.columns:
        print("   âš ï¸ ç¼ºå°‘'è§£è¯´æ–‡æœ¬'åˆ—ï¼Œè·³è¿‡")
        return

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    if not MY_API_KEY:
        print("   âŒ æ—  API Key")
        return
    client = OpenAI(
    api_key=MY_API_KEY, 
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

    # åˆ†æ‰¹å¤„ç†
    results_map = {}
    batches = [df.iloc[i:i+BATCH_SIZE] for i in range(0, len(df), BATCH_SIZE)]
    
    print(f"   ğŸš€ å…± {len(df)} æ¡è§£è¯´ï¼Œåˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡å¹¶å‘å¤„ç†...")

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_batch = {executor.submit(process_batch, client, batch): i for i, batch in enumerate(batches)}
        
        completed = 0
        for future in concurrent.futures.as_completed(future_to_batch):
            try:
                batch_res = future.result()
                # å°†ç»“æœåˆå¹¶åˆ°æ€»å­—å…¸ä¸­ (Keyæ˜¯ç´¢å¼•, Valueæ˜¯æ–‡æœ¬)
                # æ³¨æ„ï¼šJSON key æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦è½¬å› int ç´¢å¼•
                for k, v in batch_res.items():
                    results_map[int(k)] = v
                
                completed += 1
                if completed % 5 == 0:
                    print(f"      è¿›åº¦: {completed}/{len(batches)}")
            except Exception as e:
                print(f"      âŒ æ‰¹æ¬¡å¼‚å¸¸: {e}")

    # å›å¡«ç»“æœ
    # åˆ›å»ºæ–°åˆ—ï¼Œå¦‚æœæ²¡æœ‰ç»“æœåˆ™ä¿ç•™åŸæ–‡æœ¬
    new_texts = []
    for idx in df.index:
        if idx in results_map:
            new_texts.append(results_map[idx])
        else:
            new_texts.append(df.at[idx, 'è§£è¯´æ–‡æœ¬']) # å…œåº•
            
    df['åŸè§£è¯´'] = df['è§£è¯´æ–‡æœ¬']
    df['è§£è¯´æ–‡æœ¬'] = new_texts
    
    # ä¿å­˜
    new_path = csv_path.replace(".csv", "_machine_style.csv")
    df.to_csv(new_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ‰ æ¶¦è‰²å®Œæˆï¼è¾“å‡º: {new_path}")

if __name__ == "__main__":
    # è‡ªåŠ¨æ‰«æ output ç›®å½•
    base_dir = os.path.join(os.getcwd(), "data")
    target_files = []
    
    if os.path.exists(base_dir):
        # é€’å½’æŸ¥æ‰¾ final_schedule.csv
        for root, dirs, files in os.walk(base_dir):
            for file in files:
                if file == "final_schedule.csv":
                    target_files.append(os.path.join(root, file))
    
    if not target_files:
        print("âš ï¸ æœªæ‰¾åˆ° final_schedule.csvï¼Œè¯·å…ˆè¿è¡Œä¸»ç¨‹åºã€‚")
    else:
        for f in target_files:
            process_file(f)